{
    "batch_size": 64,
    "best_score": -26.882438976555633,
    "buffer_size": 100000,
    "gamma": 0.95,
    "noise": {
        "exploration_mu": 0.0,
        "exploration_sigma": 0.3,
        "exploration_theta": 0.15
    },
    "reward_function": "    def get_reward(self):\n        \"\"\"Uses current pose of sim to return reward.\"\"\"\n        # 1 - .3 * (abs(self.sim.pose[:3] - self.target_pos)).sum()\n\n        # tenho que pensar numa reward que seja mais gen\u00e9rica\n        # n\u00e3o somente para decolagem\n\n        # recompensa constante para se manter dentro do quadrante v\u00e1lido\n        reward = 1.\n\n        # penalizar pela posicao\n        # penalizar pela distancia do eixos x, y, z do target\n        # distancia maxima \u00e9 300 m em cada dire\u00e7\u00e3o\n        reward -= 0.1 * pow(self.sim.pose[:3] - self.target_pos[:3], 2).sum()\n\n        # recompensa pela velocidade do eixo\n        # (-15, 15) m/s\n        reward += 0.1 * abs(self.sim.v[:3]).sum()\n\n        # penalizar pela instabilidade dos angulos\n        # (0, 6)\n        reward -= abs(self.sim.pose[3:]).sum()\n\n        return reward\n",
    "tau": 0.01
}